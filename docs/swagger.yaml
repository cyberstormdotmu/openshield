definitions:
  openai.ChatCompletionChoice:
    properties:
      finish_reason:
        allOf:
        - $ref: '#/definitions/openai.FinishReason'
        description: |-
          FinishReason
          stop: API returned complete message,
          or a message terminated by one of the stop sequences provided via the stop parameter
          length: Incomplete model output due to max_tokens parameter or token limit
          function_call: The model decided to call a function
          content_filter: Omitted content due to a flag from our content filters
          null: API response still in progress or incomplete
      index:
        type: integer
      logprobs:
        $ref: '#/definitions/openai.LogProbs'
      message:
        $ref: '#/definitions/openai.ChatCompletionMessage'
    type: object
  openai.ChatCompletionMessage:
    properties:
      content:
        type: string
      function_call:
        $ref: '#/definitions/openai.FunctionCall'
      multiContent:
        items:
          $ref: '#/definitions/openai.ChatMessagePart'
        type: array
      name:
        description: |-
          This property isn't in the official documentation, but it's in
          the documentation for the official library for python:
          - https://github.com/openai/openai-python/blob/main/chatml.md
          - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
        type: string
      role:
        type: string
      tool_call_id:
        description: For Role=tool prompts this should be set to the ID given in the
          assistant's prior request to call a tool.
        type: string
      tool_calls:
        description: For Role=assistant prompts this may be set to the tool calls
          generated by the model, such as function calls.
        items:
          $ref: '#/definitions/openai.ToolCall'
        type: array
    type: object
  openai.ChatCompletionRequest:
    properties:
      frequency_penalty:
        type: number
      function_call:
        description: 'Deprecated: use ToolChoice instead.'
      functions:
        description: 'Deprecated: use Tools instead.'
        items:
          $ref: '#/definitions/openai.FunctionDefinition'
        type: array
      logit_bias:
        additionalProperties:
          type: integer
        description: |-
          LogitBias is must be a token id string (specified by their token ID in the tokenizer), not a word string.
          incorrect: `"logit_bias":{"You": 6}`, correct: `"logit_bias":{"1639": 6}`
          refs: https://platform.openai.com/docs/api-reference/chat/create#chat/create-logit_bias
        type: object
      logprobs:
        description: |-
          LogProbs indicates whether to return log probabilities of the output tokens or not.
          If true, returns the log probabilities of each output token returned in the content of message.
          This option is currently not available on the gpt-4-vision-preview model.
        type: boolean
      max_tokens:
        type: integer
      messages:
        items:
          $ref: '#/definitions/openai.ChatCompletionMessage'
        type: array
      model:
        type: string
      "n":
        type: integer
      parallel_tool_calls:
        description: 'Disable the default behavior of parallel tool calls by setting
          it: false.'
      presence_penalty:
        type: number
      response_format:
        $ref: '#/definitions/openai.ChatCompletionResponseFormat'
      seed:
        type: integer
      stop:
        items:
          type: string
        type: array
      stream:
        type: boolean
      stream_options:
        allOf:
        - $ref: '#/definitions/openai.StreamOptions'
        description: 'Options for streaming response. Only set this when you set stream:
          true.'
      temperature:
        type: number
      tool_choice:
        description: This can be either a string or an ToolChoice object.
      tools:
        items:
          $ref: '#/definitions/openai.Tool'
        type: array
      top_logprobs:
        description: |-
          TopLogProbs is an integer between 0 and 5 specifying the number of most likely tokens to return at each
          token position, each with an associated log probability.
          logprobs must be set to true if this parameter is used.
        type: integer
      top_p:
        type: number
      user:
        type: string
    type: object
  openai.ChatCompletionResponse:
    properties:
      choices:
        items:
          $ref: '#/definitions/openai.ChatCompletionChoice'
        type: array
      created:
        type: integer
      id:
        type: string
      model:
        type: string
      object:
        type: string
      system_fingerprint:
        type: string
      usage:
        $ref: '#/definitions/openai.Usage'
    type: object
  openai.ChatCompletionResponseFormat:
    properties:
      type:
        $ref: '#/definitions/openai.ChatCompletionResponseFormatType'
    type: object
  openai.ChatCompletionResponseFormatType:
    enum:
    - json_object
    - text
    type: string
    x-enum-varnames:
    - ChatCompletionResponseFormatTypeJSONObject
    - ChatCompletionResponseFormatTypeText
  openai.ChatMessageImageURL:
    properties:
      detail:
        $ref: '#/definitions/openai.ImageURLDetail'
      url:
        type: string
    type: object
  openai.ChatMessagePart:
    properties:
      image_url:
        $ref: '#/definitions/openai.ChatMessageImageURL'
      text:
        type: string
      type:
        $ref: '#/definitions/openai.ChatMessagePartType'
    type: object
  openai.ChatMessagePartType:
    enum:
    - text
    - image_url
    type: string
    x-enum-varnames:
    - ChatMessagePartTypeText
    - ChatMessagePartTypeImageURL
  openai.FinishReason:
    enum:
    - stop
    - length
    - function_call
    - tool_calls
    - content_filter
    - "null"
    type: string
    x-enum-varnames:
    - FinishReasonStop
    - FinishReasonLength
    - FinishReasonFunctionCall
    - FinishReasonToolCalls
    - FinishReasonContentFilter
    - FinishReasonNull
  openai.FunctionCall:
    properties:
      arguments:
        description: call function with arguments in JSON format
        type: string
      name:
        type: string
    type: object
  openai.FunctionDefinition:
    properties:
      description:
        type: string
      name:
        type: string
      parameters:
        description: |-
          Parameters is an object describing the function.
          You can pass json.RawMessage to describe the schema,
          or you can pass in a struct which serializes to the proper JSON schema.
          The jsonschema package is provided for convenience, but you should
          consider another specialized library if you require more complex schemas.
    type: object
  openai.ImageURLDetail:
    enum:
    - high
    - low
    - auto
    type: string
    x-enum-varnames:
    - ImageURLDetailHigh
    - ImageURLDetailLow
    - ImageURLDetailAuto
  openai.LogProb:
    properties:
      bytes:
        description: Omitting the field if it is null
        items:
          type: integer
        type: array
      logprob:
        type: number
      token:
        type: string
      top_logprobs:
        description: |-
          TopLogProbs is a list of the most likely tokens and their log probability, at this token position.
          In rare cases, there may be fewer than the number of requested top_logprobs returned.
        items:
          $ref: '#/definitions/openai.TopLogProbs'
        type: array
    type: object
  openai.LogProbs:
    properties:
      content:
        description: Content is a list of message content tokens with log probability
          information.
        items:
          $ref: '#/definitions/openai.LogProb'
        type: array
    type: object
  openai.Model:
    properties:
      created:
        type: integer
      id:
        type: string
      object:
        type: string
      owned_by:
        type: string
      parent:
        type: string
      permission:
        items:
          $ref: '#/definitions/openai.Permission'
        type: array
      root:
        type: string
    type: object
  openai.ModelsList:
    properties:
      data:
        items:
          $ref: '#/definitions/openai.Model'
        type: array
    type: object
  openai.Permission:
    properties:
      allow_create_engine:
        type: boolean
      allow_fine_tuning:
        type: boolean
      allow_logprobs:
        type: boolean
      allow_sampling:
        type: boolean
      allow_search_indices:
        type: boolean
      allow_view:
        type: boolean
      created:
        type: integer
      group: {}
      id:
        type: string
      is_blocking:
        type: boolean
      object:
        type: string
      organization:
        type: string
    type: object
  openai.StreamOptions:
    properties:
      include_usage:
        description: |-
          If set, an additional chunk will be streamed before the data: [DONE] message.
          The usage field on this chunk shows the token usage statistics for the entire request,
          and the choices field will always be an empty array.
          All other chunks will also include a usage field, but with a null value.
        type: boolean
    type: object
  openai.Tool:
    properties:
      function:
        $ref: '#/definitions/openai.FunctionDefinition'
      type:
        $ref: '#/definitions/openai.ToolType'
    type: object
  openai.ToolCall:
    properties:
      function:
        $ref: '#/definitions/openai.FunctionCall'
      id:
        type: string
      index:
        description: Index is not nil only in chat completion chunk object
        type: integer
      type:
        $ref: '#/definitions/openai.ToolType'
    type: object
  openai.ToolType:
    enum:
    - function
    type: string
    x-enum-varnames:
    - ToolTypeFunction
  openai.TopLogProbs:
    properties:
      bytes:
        items:
          type: integer
        type: array
      logprob:
        type: number
      token:
        type: string
    type: object
  openai.Usage:
    properties:
      completion_tokens:
        type: integer
      prompt_tokens:
        type: integer
      total_tokens:
        type: integer
    type: object
  server.ErrorResponse:
    properties:
      error:
        properties:
          code:
            type: string
          message:
            type: string
          param:
            type: string
          type:
            type: string
        type: object
    type: object
info:
  contact: {}
  description: This is the API server for OpenShield.
  title: OpenShield API
  version: "1.0"
paths:
  /openai/v1/chat/completions:
    post:
      consumes:
      - application/json
      description: Create a chat completion
      parameters:
      - description: Chat completion request
        in: body
        name: request
        required: true
        schema:
          $ref: '#/definitions/openai.ChatCompletionRequest'
      produces:
      - application/json
      responses:
        "200":
          description: OK
          schema:
            $ref: '#/definitions/openai.ChatCompletionResponse'
        "400":
          description: Bad Request
          schema:
            $ref: '#/definitions/server.ErrorResponse'
        "401":
          description: Unauthorized
          schema:
            $ref: '#/definitions/server.ErrorResponse'
        "500":
          description: Internal Server Error
          schema:
            $ref: '#/definitions/server.ErrorResponse'
      summary: Create chat completion
      tags:
      - openai
  /openai/v1/models:
    get:
      description: Get a list of available models
      produces:
      - application/json
      responses:
        "200":
          description: OK
          schema:
            $ref: '#/definitions/openai.ModelsList'
        "401":
          description: Unauthorized
          schema:
            $ref: '#/definitions/server.ErrorResponse'
        "500":
          description: Internal Server Error
          schema:
            $ref: '#/definitions/server.ErrorResponse'
      summary: List models
      tags:
      - openai
  /openai/v1/models/{model}:
    get:
      description: Get details of a specific model
      parameters:
      - description: Model ID
        in: path
        name: model
        required: true
        type: string
      produces:
      - application/json
      responses:
        "200":
          description: OK
          schema:
            $ref: '#/definitions/openai.Model'
        "401":
          description: Unauthorized
          schema:
            $ref: '#/definitions/server.ErrorResponse'
        "404":
          description: Not Found
          schema:
            $ref: '#/definitions/server.ErrorResponse'
        "500":
          description: Internal Server Error
          schema:
            $ref: '#/definitions/server.ErrorResponse'
      summary: Get model details
      tags:
      - openai
swagger: "2.0"
